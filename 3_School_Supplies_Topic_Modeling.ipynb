{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# School Supplies - Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic models have been designed specifically for the purpose of extracting various distinguishing concepts or topics from a large corpus having various types of documents where each document talks about one or more concepts. These concepts can be anything from thoughts, opinions, facts, outlooks, statements and so on. The main aim of topic modeling is to use mathematical and statistical techniques to discover hidden and latent semantic structures in a corpus. Topic modeling involves extracting features from document terms and using mathematical structures and frameworks like matrix factorization and SVD to generate clusters or groups of terms which are distinguishable from each other and these cluster of words form topics or concepts. These concepts can be used to interpret the main themes of a corpus and also make semantic connections amongst words which co-occur together frequently in various documents. There are various frameworks and algorithms to build topic models. The most popular ones include"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Latent Semantic Indexing (LSI)\n",
    " - Latent Dirichlet Allocation (LDA)\n",
    " - Non-negative Matrix Factorization (NNMF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last technique we will look at is non-negative matrix factorization (**NNMF**), which is another matrix decomposition technique similar to SVD but operates on non-negative matrices and works well for multivariate data. NNMF can be formally defined as, given a non-negative matrix V, the objective is to find two non-negative matrix factors, W and H such that when they are multiplied, they can approximately reconstruct V. \n",
    "\n",
    "  >Vâ‰ˆWH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "such that all three matrices are non-negative.\n",
    "\n",
    "To get to this approximation, we usually use a cost function like the Euclidean distance or L2 norm between two matrices or the Frobenius norm which is a slight modification of the L2 norm.\n",
    "\n",
    "This implementation is available in the NMF class in the scikit-learn decomposition module which we will be using in the section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package stopwords to /Users/amy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import gensim\n",
    "import nltk\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from  sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, regexp_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from nltk import word_tokenize\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "nltk.download('stopwords')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2, linewidth=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and View Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleaned_text_school_supplies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_ID        0\n",
       "product_ID         0\n",
       "rating             0\n",
       "review_text        0\n",
       "rating_class       0\n",
       "date_time          0\n",
       "normalized_text    1\n",
       "tokens             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting categorical to numerical \n",
    "df['rating_class'] = df['rating_class'].replace({'good': 1,'neutral': 0,'bad':0 })\n",
    "# Converting categorical to numerical \n",
    "df['rating_class'] = df['rating_class'].replace({1: 'good',0 :'bad' })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_ID</th>\n",
       "      <th>product_ID</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_text</th>\n",
       "      <th>rating_class</th>\n",
       "      <th>date_time</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0136039847</td>\n",
       "      <td>A2MVR1VMDPMNGS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I honestly believe that giving this merchant o...</td>\n",
       "      <td>bad</td>\n",
       "      <td>2014-03-03</td>\n",
       "      <td>honestly believe give merchant one star favor ...</td>\n",
       "      <td>['honestly', 'believe', 'give', 'merchant', 'o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0439504422</td>\n",
       "      <td>ASTJ36CFZRN3O</td>\n",
       "      <td>5.0</td>\n",
       "      <td>This is a standard size folder that fits stand...</td>\n",
       "      <td>good</td>\n",
       "      <td>2012-09-18</td>\n",
       "      <td>standard size folder fit standard paper 5x11 f...</td>\n",
       "      <td>['standard', 'size', 'folder', 'fit', 'standar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  customer_ID      product_ID  rating  \\\n",
       "0  0136039847  A2MVR1VMDPMNGS     1.0   \n",
       "1  0439504422   ASTJ36CFZRN3O     5.0   \n",
       "\n",
       "                                         review_text rating_class   date_time  \\\n",
       "0  I honestly believe that giving this merchant o...          bad  2014-03-03   \n",
       "1  This is a standard size folder that fits stand...         good  2012-09-18   \n",
       "\n",
       "                                     normalized_text  \\\n",
       "0  honestly believe give merchant one star favor ...   \n",
       "1  standard size folder fit standard paper 5x11 f...   \n",
       "\n",
       "                                              tokens  \n",
       "0  ['honestly', 'believe', 'give', 'merchant', 'o...  \n",
       "1  ['standard', 'size', 'folder', 'fit', 'standar...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[:8000]\n",
    "df_test = df[8000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_reviews = df['normalized_text'].values\n",
    "train_text_reviews = text_reviews [:8000]\n",
    "test_text_reviews = text_reviews [8000:]\n",
    "\n",
    "sentiments = df['rating_class'].values\n",
    "train_sentiments = sentiments[:8000]\n",
    "test_sentiments = sentiments[8000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features from positive and negative reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in this analysis is to combine all our normalized train and test reviews and separate out these reviews into positive and negative sentiment reviews. Once we do this, we will extract features from these two datasets using the TF-IDF feature vectorizer. The following snippet helps us achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Take all normalized reviews and get tf-idf features for only positive reviews\n",
    "\n",
    "positive_reviews = [review for review, sentiment in zip(text_reviews, sentiments) if sentiment == 'good']\n",
    "#### or you can use the method below #####\n",
    "# positive_reviews = df[df.rating_class == 'good']['normalized_text'].values.tolist()\n",
    "\n",
    "ptvf = TfidfVectorizer(use_idf=True, min_df=0.02, max_df=0.75, ngram_range=(1, 2), sublinear_tf=True)\n",
    "ptvf_features = ptvf.fit_transform(positive_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tf-idf features for only negative reviews\n",
    "negative_reviews = [review for review, sentiment in zip(text_reviews, sentiments) if sentiment == 'bad']\n",
    "ntvf = TfidfVectorizer(use_idf=True, min_df=0.02, max_df=0.75, ngram_range=(1, 2), sublinear_tf=True)\n",
    "ntvf_features = ntvf.fit_transform(negative_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8318, 261) (2517, 304)\n"
     ]
    }
   ],
   "source": [
    "# view feature set dimensions\n",
    "print(ptvf_features.shape, ntvf_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the preceding output dimensions, you can see that we have filtered out a lot of the features we used previously when building our classification models by making **min_df** to be 0.02 and **max_df** to be 0.75. This is to speed up the topic modeling process and remove features that either occur too much or too rarely. Letâ€™s now import the necessary dependencies for the topic modeling process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling on Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyLDAvis is a Python library for interactive topic model visualization. \n",
    "\n",
    "pyLDAvis is designed to help users interpret the topics in a topic model that has been fit to a corpus of text data. The package extracts information from a fitted LDA topic model to inform an interactive web-based visualization.\n",
    "\n",
    "The visualization is intended to be used within an IPython notebook but can also be saved to a stand-alone HTML file for easy sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename = '/Users/amy/00_Springboard/14_Capstone_Project_2/pyLDAvis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "from sklearn.decomposition import NMF\n",
    "import topic_model_utils as tmu\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "total_topics = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positive reviews / Display and visualize topics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyLDAvis_prepare as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.1, beta_loss='frobenius', init=None, l1_ratio=0.85, max_iter=500,\n",
       "  n_components=10, random_state=42, shuffle=False, solver='cd', tol=0.0001,\n",
       "  verbose=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build topic model on positive sentiment review features\n",
    "\n",
    "pos_nmf = NMF(n_components=total_topics, solver='cd', max_iter=500,\n",
    "               random_state=42, alpha=.1, l1_ratio=.85)\n",
    "pos_nmf.fit(ptvf_features)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features and component weights\n",
    "pos_feature_names = np.array(ptvf.get_feature_names())\n",
    "#pos_feature_names = (x for x in pos_feature_names if len(x) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_weights = pos_nmf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "not, one, like, get, make, look, buy, need, time, little, find, really, perfect, go, would\n",
      "\n",
      "Topic #2:\n",
      "work, well, work well, work great, fine, perfectly, printer, no, problem, use, purchase, ink, need, far, print\n",
      "\n",
      "Topic #3:\n",
      "paper, color, print, printer, use, nice, sheet, quality, like, ink, weight, project, beautiful, write, make\n",
      "\n",
      "Topic #4:\n",
      "love, calendar, gift, year, daughter, perfect, picture, buy, friend, fun, beautiful, get, son, cute, give\n",
      "\n",
      "Topic #5:\n",
      "pen, write, ink, fine, point, black, line, use, feel, pocket, nice, set, ever, like, hand\n",
      "\n",
      "Topic #6:\n",
      "use, easy, cable, easy use, cord, tie, velcro, keep, label, organize, wire, wrap, computer, make, handy\n",
      "\n",
      "Topic #7:\n",
      "product, great product, excellent, order, exactly, arrive, thank, no, need, happy, come, time, amazon, fast, receive\n",
      "\n",
      "Topic #8:\n",
      "good, quality, price, good quality, value, nice, buy, high, item, great price, ever, purchase, expect, deal, well\n",
      "\n",
      "Topic #9:\n",
      "recommend, would, highly, highly recommend, would recommend, anyone, definitely, purchase, table, sturdy, item, stand, friend, need, quality\n",
      "\n",
      "Topic #10:\n",
      "great, work great, price, great price, great product, buy, look, value, table, kid, item, money, fast, deal, really\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extract and display topics and their components\n",
    "\n",
    "pos_feature_names = np.array(ptvf.get_feature_names())\n",
    "\n",
    "feature_idxs = np.argsort(-pos_weights)[:, :15]\n",
    "\n",
    "topics = [pos_feature_names[idx] for idx in feature_idxs]\n",
    "\n",
    "for idx, topic in enumerate(topics):\n",
    "    print('Topic #'+str(idx+1)+':')\n",
    "    print(', '.join(topic))\n",
    "    print()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the positive reviews, some of topics are related with printer, print, ink, paper, pen  and computer. This shows the strong side of the company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyLDAvis.sklearn.prepare(pos_nmf, ptvf_features, ptvf, R=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative reviews / Display and visualize topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "use, would, get, one, buy, make, go, come, even, time, great, think, try, really, could\n",
      "\n",
      "Topic #2:\n",
      "money, waste, waste money, not waste, buy, time, not buy, product, no, clean, not worth, save, worth, not even, spend\n",
      "\n",
      "Topic #3:\n",
      "work, not work, well, work well, ok, stick, printer, wall, much, cartridge, surface, time, would not, product, use\n",
      "\n",
      "Topic #4:\n",
      "look, like, look like, picture, not like, nice, something, cheap, bag, expect, plastic, feel, job, item, lot\n",
      "\n",
      "Topic #5:\n",
      "pen, ink, write, dry, point, nice, buy, line, letter, cartridge, pack, every, feel, set, ever\n",
      "\n",
      "Topic #6:\n",
      "product, item, receive, order, send, amazon, return, picture, description, box, arrive, seller, no, company, package\n",
      "\n",
      "Topic #7:\n",
      "good, not good, product, price, want, deal, job, little, bad, idea, though, pretty, bit, quality, enough\n",
      "\n",
      "Topic #8:\n",
      "paper, print, thin, sheet, printer, color, ink, write, page, side, letter, use, binder, no, nothing\n",
      "\n",
      "Topic #9:\n",
      "quality, poor, cheap, expect, price, material, high, make, flimsy, low, color, case, would, recommend, not expect\n",
      "\n",
      "Topic #10:\n",
      "small, fit, size, not fit, large, cover, much, big, cord, would, quite, inch, book, keep, expect\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build topic model on negative sentiment review features\n",
    "neg_nmf = NMF(n_components=total_topics, solver='cd', max_iter=500,\n",
    "              random_state=42, alpha=.1, l1_ratio=.85)\n",
    "neg_nmf.fit(ntvf_features)      \n",
    "# extract features and component weights\n",
    "neg_feature_names = ntvf.get_feature_names()\n",
    "neg_weights = neg_nmf.components_\n",
    "# extract and display topics and their components\n",
    "neg_feature_names = np.array(ntvf.get_feature_names())\n",
    "feature_idxs = np.argsort(-neg_weights)[:, :15]\n",
    "topics = [neg_feature_names[idx] for idx in feature_idxs]\n",
    "for idx, topic in enumerate(topics):\n",
    "    print('Topic #'+str(idx+1)+':')\n",
    "    print(', '.join(topic))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el242751126817391525178815693\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el242751126817391525178815693_data = {\"mdsDat\": {\"x\": [-0.050642207695232214, -0.02547685193583527, 0.13251120344448064, -0.16086226747635812, 0.10610106712630918, 0.2467900411687952, 0.24392431163238418, -0.07505917732226632, -0.2055703542226387, -0.2117157647196384], \"y\": [0.11584446062113225, 0.2086081102166424, -0.12181632052283652, 0.1040224704439514, 0.0598602690543059, -0.16596330139704488, 0.019120010127208084, 0.21138128575322035, -0.22713251472022533, -0.2039244695763538], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [42.10654233470315, 14.248745409819074, 7.714612748819259, 7.2697368846737955, 5.699026884670063, 4.935800532349101, 4.870375460463988, 4.795443915822608, 4.524466965724294, 3.8352488629546793]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"Freq\": [251.0, 219.0, 224.0, 194.0, 155.0, 188.0, 155.0, 174.0, 122.0, 127.0, 105.0, 92.0, 89.0, 80.0, 83.0, 9.065363948751813, 9.130869506950308, 9.238611696851107, 17.059900649288647, 21.827409145787932, 23.139896220671623, 21.711274014960633, 8.492187826462393, 8.220846744736592, 8.599069751281135, 9.144982701984835, 6.505502080356773, 7.423867577682916, 9.152916011692245, 15.472794621640995, 38.16175505280262, 29.120526765941644, 26.031921821591197, 25.63404516220064, 16.339645333539647, 15.116251831476399, 14.853728951347088, 25.717798717241344, 13.665797003400733, 13.48954854734489, 27.125669541979512, 19.140371172858416, 18.027175989467967, 21.628065026083213, 23.531905515756392, 71.59638755756566, 71.0971121733665, 32.44679107483927, 49.22566768839108, 29.748871305647242, 32.989488532611375, 66.20927725410843, 61.04443337306626, 31.243881395934835, 33.20910859765756, 55.185713594331546, 31.676842374646153, 33.06087299495489, 32.29353974754415, 33.040627705429976, 77.57496669424056, 47.81022237748222, 34.04545782956607, 74.85017136303816, 28.543267813566423, 18.381074381570855, 43.81515122724776, 20.28760144566254, 20.574058839114763, 80.48276664144325, 16.55211776737768, 31.077210470172584, 16.153601392095958, 42.06723527674269, 15.311619713786838, 117.56195045568859, 23.746398968506004, 23.520957273691707, 31.72633800229624, 34.18967002672919, 24.263064509743767, 70.62805041502669, 35.49114555706827, 188.30512755106716, 159.07555689323777, 11.274440144787967, 48.474176275940096, 24.55877331346604, 25.7505920661722, 7.173110179134963, 8.093801008995403, 2.9489429648633227, 10.204274665577701, 3.0890178520897624, 24.13329173528862, 2.597798523244851, 6.183815692566358, 10.828120031675942, 5.236401598140871, 5.745889026788513, 6.394456709580282, 74.11022193565762, 127.16487071404507, 214.3070071192114, 79.3748583805375, 8.77172598232538, 19.538677791158598, 7.458025226313853, 13.689405922046578, 5.24642942599244, 11.288419091684585, 4.685698338050893, 7.680583628179115, 5.259510688488867, 6.819474653125931, 4.74069903264476, 7.918082223153665, 7.398970614203893, 6.124997921803764, 6.446399049308845, 6.816117366957744, 43.296335158851946, 155.48325451182802, 79.48344846985155, 62.22279479295984, 26.26066421450217, 12.973396125414927, 22.52654366628065, 13.631310414470196, 6.482096858833215, 6.963664423805952, 3.699340475177293, 6.4102695061117165, 5.114982587901335, 15.266960222881455, 3.654022887176763, 5.586155503223148, 7.408422268448559, 5.309110445718157, 89.4282603830968, 191.99625174802927, 60.41526345049055, 8.726944887726217, 8.311209421893599, 6.632076339201567, 20.329862625925323, 5.538476217957359, 3.0834614170045045, 14.737296165258234, 4.175924074514871, 3.3818188600903984, 2.0148103484708453, 7.744300463006402, 5.210140941724902, 2.507277822820037, 4.028621302465292, 122.97008419410986, 244.09371322406346, 5.758221771935403, 15.434015030987792, 4.843859593225027, 2.8502118840777113, 7.746685094753173, 2.769641235795213, 2.1812098587318407, 4.367827329235592, 3.3557658095366465, 2.0371511025103044, 15.876202987657285, 1.8895593580142804, 1.70150429633689, 1.7985366435357197, 2.0109654210989625, 80.37264777686624, 92.22336837925961, 54.82370203029633, 95.99759843047919, 4.667565470303709, 5.630894729180783, 3.453009731687146, 7.53027683631975, 2.5801664858859374, 4.725511257874672, 4.129463105361124, 16.51778411150084, 4.317161853271856, 10.0625006144593, 1.8518998810747858, 2.0308873096372784, 5.798213010266568, 7.144364075467879, 219.30434322460673, 23.95948087220864, 27.148002624337916, 29.61065456754375, 20.983963802480847, 5.930982800437919, 6.154897986832393, 6.685696045778845, 5.510053742785302, 2.6656460365435435, 2.515481234711257, 2.9445524656675883, 9.3277316203421, 3.483264306682565, 2.4380625969547074, 8.300265318253466, 9.225690692641358, 5.592652171611727, 3.768619658812414, 155.97053200540742, 45.59545272660084, 51.65730333513379, 10.046525040281768, 11.0184685688204, 3.2351016283311287, 5.700933346033686, 4.558416281997878, 2.976124773475298, 3.5954669575826577, 3.416892170917447, 3.288701207846476, 7.749686945945334, 2.17329356653394, 3.405150010791079, 4.423798346302156, 6.4398373977405985], \"Term\": [\"good\", \"paper\", \"work\", \"quality\", \"pen\", \"look\", \"small\", \"like\", \"not good\", \"not work\", \"money\", \"waste\", \"poor\", \"waste money\", \"fit\", \"almost\", \"school\", \"move\", \"card\", \"put\", \"piece\", \"year\", \"not really\", \"let\", \"mean\", \"metal\", \"must\", \"next\", \"pull\", \"old\", \"go\", \"thing\", \"hold\", \"take\", \"around\", \"easily\", \"less\", \"give\", \"easy\", \"stay\", \"way\", \"last\", \"hard\", \"seem\", \"first\", \"use\", \"would\", \"think\", \"make\", \"need\", \"great\", \"get\", \"one\", \"could\", \"come\", \"buy\", \"really\", \"even\", \"try\", \"time\", \"receive\", \"send\", \"description\", \"order\", \"seller\", \"product not\", \"amazon\", \"contact\", \"damage\", \"item\", \"refund\", \"arrive\", \"service\", \"return\", \"packaging\", \"product\", \"company\", \"package\", \"box\", \"picture\", \"no\", \"look like\", \"not like\", \"look\", \"like\", \"bag\", \"picture\", \"something\", \"nice\", \"job\", \"feel\", \"front\", \"plastic\", \"kind\", \"cheap\", \"guess\", \"lot\", \"expect\", \"see\", \"color\", \"item\", \"work well\", \"not work\", \"work\", \"well\", \"wall\", \"ok\", \"surface\", \"stick\", \"cable\", \"printer\", \"better\", \"cartridge\", \"fine\", \"would not\", \"lot\", \"much\", \"time\", \"try\", \"use\", \"product\", \"not fit\", \"small\", \"fit\", \"size\", \"large\", \"cord\", \"cover\", \"big\", \"inch\", \"quite\", \"standard\", \"book\", \"pocket\", \"much\", \"hole\", \"keep\", \"would\", \"expect\", \"poor\", \"quality\", \"cheap\", \"material\", \"high\", \"flimsy\", \"expect\", \"low\", \"not expect\", \"price\", \"case\", \"recommend\", \"terrible\", \"make\", \"color\", \"page\", \"would\", \"not good\", \"good\", \"deal\", \"price\", \"job\", \"idea\", \"want\", \"though\", \"pretty\", \"little\", \"bad\", \"bit\", \"product\", \"enough\", \"phone\", \"ok\", \"quality\", \"waste money\", \"waste\", \"not waste\", \"money\", \"save\", \"clean\", \"spend\", \"not buy\", \"useless\", \"not worth\", \"not even\", \"buy\", \"worth\", \"time\", \"reason\", \"apart\", \"no\", \"product\", \"paper\", \"sheet\", \"thin\", \"print\", \"printer\", \"letter\", \"side\", \"page\", \"binder\", \"weight\", \"run\", \"white\", \"color\", \"nothing\", \"brand\", \"write\", \"ink\", \"use\", \"no\", \"pen\", \"write\", \"ink\", \"point\", \"dry\", \"ever\", \"line\", \"letter\", \"reason\", \"pack\", \"every\", \"set\", \"nice\", \"terrible\", \"feel\", \"cartridge\", \"buy\"], \"Total\": [251.0, 219.0, 224.0, 194.0, 155.0, 188.0, 155.0, 174.0, 122.0, 127.0, 105.0, 92.0, 89.0, 80.0, 83.0, 9.065363948751813, 9.130869506950308, 9.238611696851107, 17.059900649288647, 21.827409145787932, 23.139896220671623, 21.711274014960633, 8.492187826462393, 8.220846744736592, 8.599069751281135, 9.144982701984835, 6.505502080356773, 7.423867577682916, 9.152916011692245, 15.472794621640995, 38.16175505280262, 29.120526765941644, 26.031921821591197, 25.63404516220064, 16.339645333539647, 15.116251831476399, 14.853728951347088, 25.86366505103644, 13.665797003400733, 13.48954854734489, 27.3661136306583, 19.317334997249446, 18.290016670373173, 22.26240493675819, 24.510078598393214, 83.63543877848625, 83.62766655030839, 35.27809392415278, 56.96996815139748, 31.99445903202247, 36.09358029537357, 83.59882903855801, 77.99053008622136, 34.249185550087205, 37.1880900073213, 79.43434095349234, 35.854133548615486, 39.96028432461025, 40.517600412417146, 60.055051655084156, 77.85294055709996, 48.30974168495663, 35.29075695118753, 78.492093066814, 30.021191698630858, 19.33634683931941, 46.583015653457764, 21.62877032943365, 22.596945227756514, 88.4996963017008, 18.495908737290765, 36.59483279918604, 19.24345246463501, 50.63835915181491, 18.48423520133627, 157.50999294409917, 29.361841637820447, 29.349825593500473, 46.086204139345774, 84.58747531591712, 63.841442829384405, 70.62805041502669, 35.49114555706827, 188.7542154377512, 174.04986742927804, 18.355348709310114, 84.58747531591712, 43.579040902061365, 52.01466297270643, 20.209820546702122, 23.649154290158908, 8.902302854634947, 32.270288216912114, 12.462814717647325, 99.86027447473693, 11.25085112050677, 27.402576602857504, 51.90091305277607, 28.1922495115684, 42.73211176997744, 88.4996963017008, 74.11022193565762, 127.42475527960457, 224.55860919732646, 110.61634336937185, 15.123386238552818, 34.832175179330214, 13.930867771362024, 31.88988921424397, 16.26756058350032, 44.14987030244311, 18.577080361013483, 30.885239236145164, 24.11833381597682, 32.551566098790865, 27.402576602857504, 51.32119868886466, 60.055051655084156, 40.517600412417146, 83.63543877848625, 157.50999294409917, 43.296335158851946, 155.48325451182802, 83.03793814825764, 67.24412127456483, 35.44069838607188, 20.356313250652835, 38.22655199788382, 26.875160952993056, 14.400868562455623, 16.042522527764778, 8.887930860791862, 15.738589593653177, 14.923787790765992, 51.32119868886466, 13.811171683202863, 26.057541946282377, 83.62766655030839, 51.90091305277607, 89.55757543898004, 194.00721716912824, 99.86027447473693, 17.266676542682912, 18.635285389322995, 16.714709992229, 51.90091305277607, 15.934484180125331, 10.483197610192317, 59.75226714189304, 23.961961502708785, 21.608376850539923, 14.660324853503656, 56.96996815139748, 42.73211176997744, 21.471671197839818, 83.62766655030839, 122.97008419410986, 251.7313474157925, 11.690938888828276, 59.75226714189304, 20.209820546702122, 13.732942268742374, 41.82175022530877, 15.988724940378498, 16.26018269476058, 32.794707113395454, 29.164879223998934, 18.437173507575253, 157.50999294409917, 21.613100804607193, 19.68648137796665, 34.832175179330214, 194.00721716912824, 80.37264777686624, 92.22336837925961, 54.82370203029633, 105.4639872665766, 10.219767729339079, 13.102984183351087, 11.640514697479638, 25.86740604308285, 10.15379698127847, 18.758372686945194, 18.99086748158994, 79.43434095349234, 24.686457847410374, 60.055051655084156, 11.727744896740766, 13.14609684760375, 63.841442829384405, 157.50999294409917, 219.30434322460673, 27.20073309070549, 35.73395013753423, 41.47260271384786, 44.14987030244311, 16.96081418877812, 19.315208075147368, 21.471671197839818, 21.919235369496914, 11.05463977396245, 10.492221441592608, 12.965190269216802, 42.73211176997744, 18.147001922434992, 13.150967090478758, 61.25644585103432, 74.36662725173838, 83.63543877848625, 63.841442829384405, 155.97053200540742, 61.25644585103432, 74.36662725173838, 18.13537417585605, 22.089073670377303, 11.477899839760415, 20.928225538206306, 16.96081418877812, 11.727744896740766, 15.879050403165095, 15.604333131867953, 22.070945762545726, 52.01466297270643, 14.660324853503656, 23.649154290158908, 30.885239236145164, 79.43434095349234], \"loglift\": [15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.865, 0.8593, 0.865, 0.865, 0.8561, 0.8558, 0.8505, 0.8361, 0.8242, 0.7095, 0.7026, 0.7813, 0.7189, 0.7922, 0.775, 0.6318, 0.62, 0.7731, 0.7518, 0.5007, 0.7411, 0.6754, 0.6381, 0.2674, 1.9449, 1.9381, 1.9126, 1.901, 1.898, 1.8978, 1.8872, 1.8845, 1.8547, 1.8535, 1.8375, 1.7851, 1.7735, 1.7631, 1.7602, 1.656, 1.7362, 1.7271, 1.5751, 1.0426, 0.9811, 2.5621, 2.5621, 2.5597, 2.4721, 2.0747, 2.0053, 1.9885, 1.859, 1.5262, 1.4898, 1.4572, 1.4107, 1.1672, 1.1419, 1.0963, 1.0734, 0.9949, 0.8786, 0.5556, -0.0655, 2.6215, 2.6194, 2.5747, 2.2896, 2.0767, 2.0433, 1.9966, 1.7758, 1.4898, 1.2576, 1.244, 1.2299, 1.0985, 1.0584, 0.867, 0.7525, 0.5275, 0.7321, 0.0585, -0.5187, 2.8649, 2.8649, 2.8211, 2.7873, 2.5651, 2.4144, 2.336, 2.186, 2.0666, 2.0303, 1.9883, 1.9667, 1.7941, 1.6525, 1.5352, 1.3249, 0.4411, 0.585, 3.0072, 2.9982, 2.5061, 2.3263, 2.2012, 2.0843, 2.0714, 1.9519, 1.7849, 1.6088, 1.2615, 1.154, 1.024, 1.0131, 0.9043, 0.8611, -0.0243, 3.022, 2.9912, 2.3138, 1.6684, 1.5935, 1.4496, 1.3358, 1.2688, 1.0132, 1.006, 0.8597, 0.8192, 0.7273, 0.585, 0.5736, 0.0584, -1.5473, 3.0375, 3.0375, 3.0375, 2.9435, 2.2538, 2.1929, 1.8223, 1.8035, 1.6675, 1.6588, 1.5117, 1.467, 1.2938, 1.2511, 1.1918, 1.1699, 0.6387, -0.0557, 3.0957, 2.9688, 2.8209, 2.7588, 2.3518, 2.045, 1.952, 1.9289, 1.7149, 1.6733, 1.6675, 1.6134, 1.5737, 1.4451, 1.4104, 1.0969, 1.0087, 0.3907, 0.266, 3.2609, 2.9657, 2.8966, 2.6703, 2.5654, 1.9946, 1.9605, 1.947, 1.8896, 1.7756, 1.7421, 1.3572, 1.3571, 1.352, 1.3229, 1.3177, 0.7485], \"logprob\": [15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -6.0524, -6.0452, -6.0335, -5.4201, -5.1737, -5.1153, -5.179, -6.1177, -6.1502, -6.1052, -6.0437, -6.3842, -6.2522, -6.0428, -5.5178, -4.615, -4.8854, -4.9975, -5.0129, -5.4633, -5.5411, -5.5586, -5.0097, -5.642, -5.655, -4.9564, -5.3051, -5.365, -5.1829, -5.0985, -3.9858, -3.9928, -4.7773, -4.3605, -4.8641, -4.7607, -4.064, -4.1453, -4.815, -4.754, -4.2462, -4.8013, -4.7585, -4.782, -4.7591, -2.8221, -3.3061, -3.6456, -2.8578, -3.8219, -4.262, -3.3934, -4.1633, -4.1493, -2.7853, -4.3668, -3.7369, -4.3912, -3.4341, -4.4447, -2.4064, -4.0059, -4.0154, -3.7162, -3.6414, -3.9844, -2.3024, -2.9905, -1.3217, -1.4904, -4.1372, -2.6787, -3.3587, -3.3113, -4.5894, -4.4687, -5.4783, -4.237, -5.4319, -3.3762, -5.6051, -4.7378, -4.1776, -4.9041, -4.8113, -4.7043, -2.1948, -1.6549, -1.133, -2.1262, -4.3289, -3.528, -4.4911, -3.8838, -4.8428, -4.0766, -4.9559, -4.4617, -4.8403, -4.5806, -4.9442, -4.4312, -4.499, -4.688, -4.6369, -4.5811, -2.4889, -1.2104, -1.8814, -2.1262, -2.9889, -3.6941, -3.1423, -3.6446, -4.3879, -4.3163, -4.9488, -4.3991, -4.6248, -3.5313, -4.9611, -4.5367, -4.2543, -4.5875, -1.6197, -0.8557, -2.0119, -3.9468, -3.9956, -4.2213, -3.1011, -4.4015, -4.9871, -3.4228, -4.6838, -4.8948, -5.4127, -4.0662, -4.4626, -5.194, -4.7198, -1.2879, -0.6023, -4.3492, -3.3633, -4.5221, -5.0524, -4.0526, -5.0811, -5.32, -4.6256, -4.8892, -5.3883, -3.335, -5.4635, -5.5683, -5.5129, -5.4012, -1.6977, -1.5601, -2.0802, -1.52, -4.5437, -4.3561, -4.8451, -4.0654, -5.1365, -4.5314, -4.6662, -3.2799, -4.6217, -3.7755, -5.4681, -5.3759, -4.3268, -4.118, -0.6357, -2.8498, -2.7249, -2.638, -2.9824, -4.246, -4.2089, -4.1262, -4.3196, -5.0457, -5.1037, -4.9462, -3.7932, -4.7782, -5.135, -3.9099, -3.8042, -4.3047, -4.6995, -0.8112, -2.0411, -1.9163, -3.5537, -3.4613, -4.6868, -4.1203, -4.3439, -4.7703, -4.5812, -4.6322, -4.6704, -3.8132, -5.0847, -4.6356, -4.3739, -3.9984]}, \"token.table\": {\"Topic\": [1, 1, 2, 1, 8, 1, 1, 2, 1, 2, 6, 7, 1, 3, 1, 4, 1, 5, 1, 9, 1, 5, 7, 1, 5, 1, 2, 3, 1, 9, 1, 7, 8, 10, 1, 4, 5, 1, 1, 2, 4, 8, 10, 1, 6, 1, 3, 6, 8, 1, 8, 1, 2, 3, 5, 6, 9, 1, 5, 8, 1, 2, 1, 2, 4, 1, 5, 1, 4, 8, 1, 5, 1, 2, 1, 2, 7, 2, 5, 1, 4, 8, 10, 1, 1, 1, 7, 1, 2, 5, 8, 1, 10, 1, 10, 1, 2, 3, 5, 6, 10, 1, 3, 10, 1, 4, 6, 9, 10, 1, 1, 5, 1, 3, 6, 1, 3, 1, 2, 3, 4, 1, 1, 1, 3, 7, 1, 4, 1, 3, 1, 1, 6, 9, 1, 1, 5, 1, 3, 7, 1, 2, 5, 1, 2, 4, 6, 9, 10, 2, 3, 5, 1, 3, 7, 1, 5, 1, 3, 1, 2, 5, 9, 1, 1, 1, 1, 9, 10, 1, 3, 10, 1, 3, 9, 10, 1, 5, 7, 9, 3, 3, 1, 3, 4, 1, 6, 1, 6, 1, 6, 1, 1, 1, 2, 8, 1, 1, 3, 4, 5, 7, 1, 1, 2, 1, 1, 3, 7, 9, 10, 1, 2, 8, 9, 1, 8, 1, 3, 5, 8, 1, 6, 5, 7, 3, 1, 8, 4, 1, 3, 8, 1, 2, 3, 4, 8, 9, 1, 4, 5, 7, 1, 1, 2, 10, 1, 2, 1, 2, 10, 1, 2, 1, 2, 3, 1, 6, 9, 9, 10, 1, 7, 2, 3, 5, 1, 1, 3, 1, 5, 1, 10, 6, 1, 7, 1, 6, 7, 1, 8, 9, 1, 2, 4, 8, 9, 1, 2, 4, 6, 7, 8, 1, 2, 1, 1, 6, 7, 1, 5, 9, 1, 3, 4, 7, 1, 2, 8, 10, 2, 1, 6, 1, 2, 1, 2, 5, 1, 9, 1, 8, 1, 1, 3, 1, 1, 2, 2, 1, 2, 1, 2, 10, 1, 9, 1, 9, 1, 5, 5, 1, 3, 1, 8, 1, 5, 1, 1, 4, 1, 4, 1, 1, 2, 6, 10, 1, 9, 1, 1, 5, 10, 1, 7, 1, 2, 4, 8, 1, 4, 8, 1, 4, 9, 1, 4, 8, 1, 4, 1, 2, 3, 7, 10, 8, 8, 1, 1, 9, 1, 3, 4, 10, 1, 2, 9, 1, 4, 4, 1, 3, 8, 10, 1, 4, 5, 6, 1, 4, 1, 9, 10, 1], \"Freq\": [0.9927897049559922, 0.06440115475386395, 0.9445502697233379, 0.8367502634065154, 0.15213641152845733, 0.9792134206950948, 0.16395757381718262, 0.8471141313887768, 0.6857563114316818, 0.17143907785792045, 0.03428781557158409, 0.10286344671475227, 0.3813602296996675, 0.5992803609566203, 0.753616807804788, 0.2691488599302814, 0.48371803327013024, 0.520927112752448, 0.7299524700695447, 0.27373217627607926, 0.7593354802593707, 0.16271474576986514, 0.10847649717991009, 0.5718428545610837, 0.38122856970738916, 0.3037785441749497, 0.6943509581141707, 0.021698467441067834, 0.8364403868034884, 0.152080070327907, 0.6923957489897438, 0.01258901361799534, 0.21401323150592078, 0.07553408170797205, 0.6147203170795434, 0.3073601585397717, 0.06147203170795435, 0.9964888043301035, 0.032377926308231234, 0.48566889462346846, 0.25902341046584987, 0.06475585261646247, 0.12951170523292493, 0.8346562111678167, 0.16693124223356334, 0.12016790523678973, 0.24033581047357946, 0.6008395261839486, 0.030041976309197433, 0.5342294474333824, 0.4579109549428992, 0.3276224698503214, 0.18721283991446938, 0.14040962993585204, 0.023401604989308673, 0.11700802494654336, 0.21061444490377806, 0.8873808790261403, 0.026890329667458798, 0.08067098900237639, 0.20434685514656242, 0.8173874205862497, 0.04623471352132967, 0.9246942704265935, 0.04623471352132967, 0.3438736628684719, 0.6386225167557334, 0.9051310126678638, 0.029197774602189155, 0.05839554920437831, 0.41855723741146583, 0.6016760287789821, 0.08850753851203477, 0.9293291543763651, 0.42768164708977663, 0.08553632941795533, 0.5132179765077319, 0.9634250703952639, 0.028336031482213645, 0.316898757478789, 0.1358137532051953, 0.045271251068398424, 0.4979837617523827, 0.9923094803677239, 1.0244554339945267, 0.9253646749168295, 0.09253646749168294, 0.8258199499265416, 0.025024846967470957, 0.12512423483735477, 0.025024846967470957, 0.69699161969399, 0.26137185738524626, 0.7690171632835111, 0.19225429082087778, 0.2504772890369153, 0.019267483772070406, 0.21194232149277445, 0.09633741886035203, 0.38534967544140813, 0.03853496754414081, 0.5074177221209786, 0.3382784814139857, 0.12685443053024464, 0.5804712757863031, 0.20731116992367968, 0.08292446796947188, 0.04146223398473594, 0.08292446796947188, 0.9791890264102763, 0.04817075290162333, 0.9513723698070607, 0.5384478704197846, 0.05982754115775385, 0.41879278810427695, 0.6739829118345626, 0.3369914559172813, 0.7894847422989506, 0.10765701031349327, 0.03588567010449776, 0.07177134020899552, 1.0052712927071448, 0.9957613308775027, 0.027807422761845717, 0.00397248896597796, 0.969287307698622, 0.9142900130699946, 0.08311727391545405, 0.7999394804536899, 0.2666464934845633, 0.9841434441750426, 0.48295477165895784, 0.42929313036351807, 0.05366164129543976, 0.9987737431830822, 0.7240515308460029, 0.2896206123384012, 0.728176075039728, 0.0728176075039728, 0.21845282251191842, 0.48608179219478737, 0.06944025602782676, 0.4166415361669606, 0.08068135159188283, 0.013446891931980473, 0.08068135159188283, 0.013446891931980473, 0.12102202738782426, 0.6992383804629846, 0.9039579042991874, 0.06779684282243906, 0.022598947607479686, 0.39584715665896675, 0.3463662620765959, 0.24740447291185422, 0.767532104188108, 0.2302596312564324, 0.7221482629647069, 0.24071608765490232, 0.1975130378003777, 0.028216148257196812, 0.7336198546871172, 0.028216148257196812, 0.9835725270957598, 1.0098474294994892, 0.9731357667167327, 0.3537566023198234, 0.3537566023198234, 0.2947971685998529, 0.08043671740048089, 0.9135312904768901, 0.005745479814320063, 0.5733883160850379, 0.14334707902125948, 0.04778235967375316, 0.28669415804251897, 0.7318254106375863, 0.06098545088646552, 0.12197090177293105, 0.03049272544323276, 0.9960042458601411, 1.0052663153348798, 0.5838866991190726, 0.21895751216965226, 0.1824645934747102, 0.6275697341036455, 0.3765418404621873, 0.860102288802105, 0.1404248634778947, 0.463320198315185, 0.5212352231045831, 1.0466248396995654, 0.9841462027092334, 0.06637336764355793, 0.018963819326730837, 0.9102633276830802, 0.9741723426981528, 0.506613264386619, 0.0194851255533315, 0.155881004426652, 0.29227688329997253, 0.0194851255533315, 1.0760122606272549, 0.9376623611599039, 0.06251082407732693, 0.9429047496809998, 0.3076055689987966, 0.4998590496230445, 0.019225348062424787, 0.038450696124849575, 0.1538027844993983, 0.4699141916352782, 0.3759313533082225, 0.09398283832705563, 0.06265522555137043, 0.6958563982032263, 0.309269510312545, 0.5265688894777537, 0.1579706668433261, 0.10531377789555074, 0.21062755579110148, 0.6677351949556146, 0.28617222640954915, 0.9931556526028195, 1.0002432771034206, 0.9861614622644814, 0.9420422820926436, 1.0032157253737852, 0.9966666188318546, 0.6397143398452333, 0.10661905664087222, 0.26654764160218053, 0.4408441699733147, 0.11021104249332868, 0.11021104249332868, 0.05510552124666434, 0.11021104249332868, 0.165316563739993, 0.3157999735407715, 0.5741817700741301, 0.057418177007413, 0.057418177007413, 0.9694434888329919, 0.7821462417624587, 0.1923310430563423, 0.012822069537089488, 0.05096054702726709, 0.955510256761258, 0.31488029025988695, 0.44083240636384174, 0.25190423220790953, 0.20443051632063877, 0.8177220652825551, 0.05410015557082436, 0.8115023335623653, 0.10820031114164871, 0.5588759202500863, 0.13971898006252156, 0.326010953479217, 0.9986122334827859, 1.0001889330902043, 0.9143330214482015, 0.10159255793868906, 0.40195076012160047, 0.5674598966422595, 0.023644162360094145, 0.9939543280861108, 0.6817416644103695, 0.30988257473198616, 0.6700711736324368, 0.3350355868162184, 0.44112682332469016, 0.5514085291558627, 0.9937741119470129, 0.8609989360396999, 0.12299984800567142, 0.48533723299793835, 0.2510364998265198, 0.2510364998265198, 0.26523534285749223, 0.024112303896135657, 0.7233691168840697, 0.0679503694903038, 0.18120098530747678, 0.24915135479778058, 0.022650123163434597, 0.47565258643212655, 0.044441624744941254, 0.7491588171290097, 0.044441624744941254, 0.019046410604974823, 0.10158085655986572, 0.044441624744941254, 0.05171607689445011, 0.930889384100102, 0.9832931918640022, 1.0079070700997683, 0.9896539046411947, 0.010308894840012446, 0.4363403565670615, 0.4363403565670615, 0.12466867330487472, 0.8925051823274553, 0.05578157389546596, 0.02789078694773298, 0.02789078694773298, 0.4263394236507941, 0.08526788473015882, 0.17053576946031765, 0.25580365419047646, 1.0018889388358065, 0.8330102776576779, 0.13883504627627966, 0.10813202143280877, 0.9191221821788745, 0.15798300209562133, 0.8294107610020119, 0.019747875261952666, 0.7624696108954487, 0.2859261040857933, 0.587097491734093, 0.4892479097784108, 0.9856673554637166, 0.8158270588007597, 0.17735370843494774, 0.9882130911955103, 0.03330980362267251, 0.9659843050575028, 0.9935884218347398, 0.15589718141863068, 0.8314516342326971, 0.634318082723846, 0.2265421724013736, 0.13592530344082415, 0.1102911450950968, 0.8823291607607744, 0.6730447815743147, 0.3106360530342991, 0.07435594227760778, 0.9220136842423364, 0.9968919192401439, 0.4359894023987404, 0.5736702663141321, 0.687254834335816, 0.257720562875931, 0.5625606317502936, 0.45004850540023483, 0.9637090488516573, 0.5644422242759032, 0.43901061888125803, 0.43069822343259373, 0.5024812606713593, 1.0142761251875685, 0.477479187531583, 0.27284525001804744, 0.13642262500902372, 0.13642262500902372, 0.25186132418499624, 0.7555839725549888, 0.9958611062598425, 0.9070784852718909, 0.056692405329493184, 0.028346202664746592, 0.8130729653850842, 0.18763222278117328, 0.5494958224252277, 0.16651388558340233, 0.11655971990838163, 0.16651388558340233, 0.7897802356082564, 0.14808379417654807, 0.049361264725516024, 0.8608790848900375, 0.07173992374083646, 0.07173992374083646, 0.5909119525496498, 0.09848532542494162, 0.2954559762748249, 0.3967365446704448, 0.5951048170056672, 0.6216860810446401, 0.04782200623420308, 0.1195550155855077, 0.19128802493681232, 0.02391100311710154, 0.9975779633385214, 0.9953635000566262, 0.9866216432629241, 0.7236780359720815, 0.27137926348953056, 0.2621674077867747, 0.009040255440923265, 0.7141801798329379, 0.009040255440923265, 0.6170368373994763, 0.1542592093498691, 0.2313888140248036, 0.04453180412785999, 0.9529806083362037, 0.9985127296507988, 0.7291447040016805, 0.08101607822240894, 0.1620321564448179, 0.04050803911120447, 0.849001328493222, 0.01195776519004538, 0.08370435633031766, 0.04783106076018152, 0.7987326914192855, 0.21504341692057688, 0.11427368830739637, 0.13059850092273873, 0.7509413803057476, 1.0132984358651829], \"Term\": [\"almost\", \"amazon\", \"amazon\", \"apart\", \"apart\", \"around\", \"arrive\", \"arrive\", \"bad\", \"bad\", \"bad\", \"bad\", \"bag\", \"bag\", \"better\", \"better\", \"big\", \"big\", \"binder\", \"binder\", \"bit\", \"bit\", \"bit\", \"book\", \"book\", \"box\", \"box\", \"box\", \"brand\", \"brand\", \"buy\", \"buy\", \"buy\", \"buy\", \"cable\", \"cable\", \"cable\", \"card\", \"cartridge\", \"cartridge\", \"cartridge\", \"cartridge\", \"cartridge\", \"case\", \"case\", \"cheap\", \"cheap\", \"cheap\", \"cheap\", \"clean\", \"clean\", \"color\", \"color\", \"color\", \"color\", \"color\", \"color\", \"come\", \"come\", \"come\", \"company\", \"company\", \"contact\", \"contact\", \"contact\", \"cord\", \"cord\", \"could\", \"could\", \"could\", \"cover\", \"cover\", \"damage\", \"damage\", \"deal\", \"deal\", \"deal\", \"description\", \"description\", \"dry\", \"dry\", \"dry\", \"dry\", \"easily\", \"easy\", \"enough\", \"enough\", \"even\", \"even\", \"even\", \"even\", \"ever\", \"ever\", \"every\", \"every\", \"expect\", \"expect\", \"expect\", \"expect\", \"expect\", \"expect\", \"feel\", \"feel\", \"feel\", \"fine\", \"fine\", \"fine\", \"fine\", \"fine\", \"first\", \"fit\", \"fit\", \"flimsy\", \"flimsy\", \"flimsy\", \"front\", \"front\", \"get\", \"get\", \"get\", \"get\", \"give\", \"go\", \"good\", \"good\", \"good\", \"great\", \"great\", \"guess\", \"guess\", \"hard\", \"high\", \"high\", \"high\", \"hold\", \"hole\", \"hole\", \"idea\", \"idea\", \"idea\", \"inch\", \"inch\", \"inch\", \"ink\", \"ink\", \"ink\", \"ink\", \"ink\", \"ink\", \"item\", \"item\", \"item\", \"job\", \"job\", \"job\", \"keep\", \"keep\", \"kind\", \"kind\", \"large\", \"large\", \"large\", \"large\", \"last\", \"less\", \"let\", \"letter\", \"letter\", \"letter\", \"like\", \"like\", \"like\", \"line\", \"line\", \"line\", \"line\", \"little\", \"little\", \"little\", \"little\", \"look\", \"look like\", \"lot\", \"lot\", \"lot\", \"low\", \"low\", \"make\", \"make\", \"material\", \"material\", \"mean\", \"metal\", \"money\", \"money\", \"money\", \"move\", \"much\", \"much\", \"much\", \"much\", \"much\", \"must\", \"need\", \"need\", \"next\", \"nice\", \"nice\", \"nice\", \"nice\", \"nice\", \"no\", \"no\", \"no\", \"no\", \"not buy\", \"not buy\", \"not even\", \"not even\", \"not even\", \"not even\", \"not expect\", \"not expect\", \"not fit\", \"not good\", \"not like\", \"not really\", \"not waste\", \"not work\", \"not worth\", \"not worth\", \"not worth\", \"nothing\", \"nothing\", \"nothing\", \"nothing\", \"nothing\", \"nothing\", \"ok\", \"ok\", \"ok\", \"ok\", \"old\", \"one\", \"one\", \"one\", \"order\", \"order\", \"pack\", \"pack\", \"pack\", \"package\", \"package\", \"packaging\", \"packaging\", \"packaging\", \"page\", \"page\", \"page\", \"paper\", \"pen\", \"phone\", \"phone\", \"picture\", \"picture\", \"picture\", \"piece\", \"plastic\", \"plastic\", \"pocket\", \"pocket\", \"point\", \"point\", \"poor\", \"pretty\", \"pretty\", \"price\", \"price\", \"price\", \"print\", \"print\", \"print\", \"printer\", \"printer\", \"printer\", \"printer\", \"printer\", \"product\", \"product\", \"product\", \"product\", \"product\", \"product\", \"product not\", \"product not\", \"pull\", \"put\", \"quality\", \"quality\", \"quite\", \"quite\", \"quite\", \"really\", \"really\", \"really\", \"really\", \"reason\", \"reason\", \"reason\", \"reason\", \"receive\", \"recommend\", \"recommend\", \"refund\", \"refund\", \"return\", \"return\", \"return\", \"run\", \"run\", \"save\", \"save\", \"school\", \"see\", \"see\", \"seem\", \"seller\", \"seller\", \"send\", \"service\", \"service\", \"set\", \"set\", \"set\", \"sheet\", \"sheet\", \"side\", \"side\", \"size\", \"size\", \"small\", \"something\", \"something\", \"spend\", \"spend\", \"standard\", \"standard\", \"stay\", \"stick\", \"stick\", \"surface\", \"surface\", \"take\", \"terrible\", \"terrible\", \"terrible\", \"terrible\", \"thin\", \"thin\", \"thing\", \"think\", \"think\", \"think\", \"though\", \"though\", \"time\", \"time\", \"time\", \"time\", \"try\", \"try\", \"try\", \"use\", \"use\", \"use\", \"useless\", \"useless\", \"useless\", \"wall\", \"wall\", \"want\", \"want\", \"want\", \"want\", \"want\", \"waste\", \"waste money\", \"way\", \"weight\", \"weight\", \"well\", \"well\", \"well\", \"well\", \"white\", \"white\", \"white\", \"work\", \"work\", \"work well\", \"worth\", \"worth\", \"worth\", \"worth\", \"would\", \"would\", \"would\", \"would\", \"would not\", \"would not\", \"write\", \"write\", \"write\", \"year\"]}, \"R\": 15, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 6, 4, 3, 10, 9, 7, 2, 8, 5]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el242751126817391525178815693\", ldavis_el242751126817391525178815693_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el242751126817391525178815693\", ldavis_el242751126817391525178815693_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el242751126817391525178815693\", ldavis_el242751126817391525178815693_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "0     -0.050642  0.115844       1        1  42.106542\n",
       "5     -0.025477  0.208608       2        1  14.248745\n",
       "3      0.132511 -0.121816       3        1   7.714613\n",
       "2     -0.160862  0.104022       4        1   7.269737\n",
       "9      0.106101  0.059860       5        1   5.699027\n",
       "8      0.246790 -0.165963       6        1   4.935801\n",
       "6      0.243924  0.019120       7        1   4.870375\n",
       "1     -0.075059  0.211381       8        1   4.795444\n",
       "7     -0.205570 -0.227133       9        1   4.524467\n",
       "4     -0.211716 -0.203924      10        1   3.835249, topic_info=    Category        Freq         Term       Total  loglift  logprob\n",
       "93   Default  251.000000         good  251.000000  15.0000  15.0000\n",
       "186  Default  219.000000        paper  219.000000  14.0000  14.0000\n",
       "296  Default  224.000000         work  224.000000  13.0000  13.0000\n",
       "211  Default  194.000000      quality  194.000000  12.0000  12.0000\n",
       "189  Default  155.000000          pen  155.000000  11.0000  11.0000\n",
       "135  Default  188.000000         look  188.000000  10.0000  10.0000\n",
       "251  Default  155.000000        small  155.000000   9.0000   9.0000\n",
       "131  Default  174.000000         like  174.000000   8.0000   8.0000\n",
       "164  Default  122.000000     not good  122.000000   7.0000   7.0000\n",
       "170  Default  127.000000     not work  127.000000   6.0000   6.0000\n",
       "148  Default  105.000000        money  105.000000   5.0000   5.0000\n",
       "286  Default   92.000000        waste   92.000000   4.0000   4.0000\n",
       "198  Default   89.000000         poor   89.000000   3.0000   3.0000\n",
       "287  Default   80.000000  waste money   80.000000   2.0000   2.0000\n",
       "84   Default   83.000000          fit   83.000000   1.0000   1.0000\n",
       "3     Topic1    9.065364       almost    9.065364   0.8650  -6.0524\n",
       "233   Topic1    9.130870       school    9.130870   0.8650  -6.0452\n",
       "150   Topic1    9.238612         move    9.238612   0.8650  -6.0335\n",
       "34    Topic1   17.059901         card   17.059901   0.8650  -5.4201\n",
       "210   Topic1   21.827409          put   21.827409   0.8650  -5.1737\n",
       "193   Topic1   23.139896        piece   23.139896   0.8650  -5.1153\n",
       "303   Topic1   21.711274         year   21.711274   0.8650  -5.1790\n",
       "166   Topic1    8.492188   not really    8.492188   0.8650  -6.1177\n",
       "128   Topic1    8.220847          let    8.220847   0.8650  -6.1502\n",
       "146   Topic1    8.599070         mean    8.599070   0.8650  -6.1052\n",
       "147   Topic1    9.144983        metal    9.144983   0.8650  -6.0437\n",
       "152   Topic1    6.505502         must    6.505502   0.8650  -6.3842\n",
       "156   Topic1    7.423868         next    7.423868   0.8650  -6.2522\n",
       "207   Topic1    9.152916         pull    9.152916   0.8650  -6.0428\n",
       "178   Topic1   15.472795          old   15.472795   0.8650  -5.5178\n",
       "..       ...         ...          ...         ...      ...      ...\n",
       "248   Topic9    6.154898         side   19.315208   1.9520  -4.2089\n",
       "185   Topic9    6.685696         page   21.471671   1.9289  -4.1262\n",
       "19    Topic9    5.510054       binder   21.919235   1.7149  -4.3196\n",
       "290   Topic9    2.665646       weight   11.054640   1.6733  -5.0457\n",
       "230   Topic9    2.515481          run   10.492221   1.6675  -5.1037\n",
       "292   Topic9    2.944552        white   12.965190   1.6134  -4.9462\n",
       "42    Topic9    9.327732        color   42.732112   1.5737  -3.7932\n",
       "173   Topic9    3.483264      nothing   18.147002   1.4451  -4.7782\n",
       "26    Topic9    2.438063        brand   13.150967   1.4104  -5.1350\n",
       "301   Topic9    8.300265        write   61.256446   1.0969  -3.9099\n",
       "113   Topic9    9.225691          ink   74.366627   1.0087  -3.8042\n",
       "282   Topic9    5.592652          use   83.635439   0.3907  -4.3047\n",
       "158   Topic9    3.768620           no   63.841443   0.2660  -4.6995\n",
       "189  Topic10  155.970532          pen  155.970532   3.2609  -0.8112\n",
       "301  Topic10   45.595453        write   61.256446   2.9657  -2.0411\n",
       "113  Topic10   51.657303          ink   74.366627   2.8966  -1.9163\n",
       "197  Topic10   10.046525        point   18.135374   2.6703  -3.5537\n",
       "65   Topic10   11.018469          dry   22.089074   2.5654  -3.4613\n",
       "72   Topic10    3.235102         ever   11.477900   1.9946  -4.6868\n",
       "132  Topic10    5.700933         line   20.928226   1.9605  -4.1203\n",
       "129  Topic10    4.558416       letter   16.960814   1.9470  -4.3439\n",
       "217  Topic10    2.976125       reason   11.727745   1.8896  -4.7703\n",
       "182  Topic10    3.595467         pack   15.879050   1.7756  -4.5812\n",
       "73   Topic10    3.416892        every   15.604333   1.7421  -4.6322\n",
       "241  Topic10    3.288701          set   22.070946   1.3572  -4.6704\n",
       "157  Topic10    7.749687         nice   52.014663   1.3571  -3.8132\n",
       "269  Topic10    2.173294     terrible   14.660325   1.3520  -5.0847\n",
       "80   Topic10    3.405150         feel   23.649154   1.3229  -4.6356\n",
       "35   Topic10    4.423798    cartridge   30.885239   1.3177  -4.3739\n",
       "28   Topic10    6.439837          buy   79.434341   0.7485  -3.9984\n",
       "\n",
       "[227 rows x 6 columns], token_table=      Topic      Freq         Term\n",
       "term                              \n",
       "3         1  0.992790       almost\n",
       "7         1  0.064401       amazon\n",
       "7         2  0.944550       amazon\n",
       "10        1  0.836750        apart\n",
       "10        8  0.152136        apart\n",
       "11        1  0.979213       around\n",
       "12        1  0.163958       arrive\n",
       "12        2  0.847114       arrive\n",
       "15        1  0.685756          bad\n",
       "15        2  0.171439          bad\n",
       "15        6  0.034288          bad\n",
       "15        7  0.102863          bad\n",
       "16        1  0.381360          bag\n",
       "16        3  0.599280          bag\n",
       "17        1  0.753617       better\n",
       "17        4  0.269149       better\n",
       "18        1  0.483718          big\n",
       "18        5  0.520927          big\n",
       "19        1  0.729952       binder\n",
       "19        9  0.273732       binder\n",
       "20        1  0.759335          bit\n",
       "20        5  0.162715          bit\n",
       "20        7  0.108476          bit\n",
       "23        1  0.571843         book\n",
       "23        5  0.381229         book\n",
       "25        1  0.303779          box\n",
       "25        2  0.694351          box\n",
       "25        3  0.021698          box\n",
       "26        1  0.836440        brand\n",
       "26        9  0.152080        brand\n",
       "...     ...       ...          ...\n",
       "285      10  0.023911         want\n",
       "286       8  0.997578        waste\n",
       "287       8  0.995364  waste money\n",
       "288       1  0.986622          way\n",
       "290       1  0.723678       weight\n",
       "290       9  0.271379       weight\n",
       "291       1  0.262167         well\n",
       "291       3  0.009040         well\n",
       "291       4  0.714180         well\n",
       "291      10  0.009040         well\n",
       "292       1  0.617037        white\n",
       "292       2  0.154259        white\n",
       "292       9  0.231389        white\n",
       "296       1  0.044532         work\n",
       "296       4  0.952981         work\n",
       "297       4  0.998513    work well\n",
       "298       1  0.729145        worth\n",
       "298       3  0.081016        worth\n",
       "298       8  0.162032        worth\n",
       "298      10  0.040508        worth\n",
       "299       1  0.849001        would\n",
       "299       4  0.011958        would\n",
       "299       5  0.083704        would\n",
       "299       6  0.047831        would\n",
       "300       1  0.798733    would not\n",
       "300       4  0.215043    would not\n",
       "301       1  0.114274        write\n",
       "301       9  0.130599        write\n",
       "301      10  0.750941        write\n",
       "303       1  1.013298         year\n",
       "\n",
       "[414 rows x 3 columns], R=15, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[1, 6, 4, 3, 10, 9, 7, 2, 8, 5])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.sklearn.prepare(neg_nmf, ntvf_features, ntvf, R=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
